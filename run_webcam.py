


"""
python run_webcam.py --use_region -b obj_000014 -m <path/to/obj/dir>
"""

import cv2
import argparse
import numpy as np
import quaternion
from pathlib import Path

import pym3t

def inv_SE3(T):
    """
    Inverse of an SE(3) 4x4 array
    """
    Tinv = np.eye(4)
    Tinv[:3,:3] = T[:3,:3].T
    Tinv[:3,3] = -T[:3,:3].T@T[:3,3]
    return Tinv


def tq_to_SE3(t, q):
    """
    t: translation as list or array
    q: quaternion as list or array, expected order: xyzw
    out: 4x4 array representing the SE(3) transformation
    """
    T = np.eye(4)
    T[:3,3] = t
    # np.quaternion constructor uses wxyz convention
    quat = np.quaternion(q[3], q[0], q[1], q[2]).normalized()
    T[:3,:3] = quaternion.as_rotation_matrix(quat)
    return T


def parse_script_input():
    parser = argparse.ArgumentParser(
        prog='run_image_per_image',
        description='Run the m3t tracker image per image reading from rgb/depth folder'
    )

    parser.add_argument('-b', '--body_name',  dest='body_name',  type=str, required=True, help='Name of the object to track. need to match')
    parser.add_argument('-m', '--models_dir', dest='models_dir', type=str, required=True, help='Path to directory where object model file {body_name}.obj is stored')
    parser.add_argument('--fov', dest='fov', type=float, default=50.0, help='Approximate horizontal field of view of the webcam in degrees')
    parser.add_argument('--scale_geometry', dest='scale_geometry', default=0.001, type=float, required=False, help='Scale factor to convert model geometry to meters.')
    parser.add_argument('--tmp_dir',    dest='tmp_dir',    type=str, default='tmp', help='Directory to store preprocessing files generated by the tracker.')
    parser.add_argument('--use_region', dest='use_region', action='store_true', default=False)
    parser.add_argument('--use_texture', dest='use_texture', action='store_true', default=False)
    parser.add_argument('--model_occlusions', dest='model_occlusions', action='store_true', default=False)

    return parser.parse_args()

args = parse_script_input()
tmp_dir = Path(args.tmp_dir)
tmp_dir.mkdir(exist_ok=True)

# synchronize_cameras: to be able to print elapsed time
tracker = pym3t.Tracker('tracker', synchronize_cameras=False)
renderer_geometry = pym3t.RendererGeometry('renderer geometry')

# revover one image to get the camera resolution
vid = cv2.VideoCapture(0) 
ret, frame = vid.read()
height, width, _ = frame.shape

# Idea from https://learnopencv.com/approximate-focal-length-for-webcams-and-cell-phone-cameras/
f_approx = (width/2)/np.tan(np.deg2rad(args.fov)/2)
intrinsics_approx = {
    'fu': f_approx,
	'fv': f_approx,
	'ppu': width/2,
	'ppv': height/2,
	'width': width,
	'height': height,
}
color_camera = pym3t.DummyColorCamera('cam_color')
color_camera.color2depth_pose = np.eye(4)
color_camera.intrinsics = pym3t.Intrinsics(**intrinsics_approx)

# Most time is spent on rendering (tested without GPU: ~15 ms for both, 8 for color only)
color_viewer = pym3t.NormalColorViewer('color_viewer', color_camera, renderer_geometry)
tracker.AddViewer(color_viewer)

# Setup body model and properties
obj_model_path = Path(args.models_dir) / f'{args.body_name}.obj'
if not obj_model_path.exists(): raise ValueError(f'{obj_model_path} is a wrong path')
print(f'Loading object {obj_model_path}')
body = pym3t.Body(
    name=args.body_name,
    geometry_path=obj_model_path.as_posix(),
    geometry_unit_in_meter=args.scale_geometry,
    geometry_counterclockwise=1,
    geometry_enable_culling=1,
    geometry2body_pose=np.eye(4)
)
renderer_geometry.AddBody(body)

# Set up link (m3t handles polyarticulated systems, here we have only one link corresponding to the object)
link = pym3t.Link(args.body_name + '_link', body)

# Shared renderer between region and texture 
if args.model_occlusions and (args.use_region or args.use_texture):
    focused_color_depth_renderer = pym3t.FocusedBasicDepthRenderer('focused_color_depth_renderer', renderer_geometry, color_camera)
    focused_color_depth_renderer.AddReferencedBody(body)

# Region Modality
if args.use_region:
    region_model_path = tmp_dir / (args.body_name + '_region_model.bin')
    region_model = pym3t.RegionModel(args.body_name + '_region_model', body, region_model_path.as_posix())
    region_modality = pym3t.RegionModality(args.body_name + '_region_modality', body, color_camera, region_model)
    if args.model_occlusions:
        region_modality.ModelOcclusions(focused_color_depth_renderer)
    link.AddModality(region_modality)

# Texture Modality
if args.use_texture:
    # Texture modality does not require a model contrary to region and depth (for sparse view precomputations)
    color_silhouette_renderer = pym3t.FocusedSilhouetteRenderer('color_silhouette_renderer', renderer_geometry, color_camera)
    color_silhouette_renderer.AddReferencedBody(body)
    texture_modality = pym3t.TextureModality(args.body_name + '_texture_modality', body, color_camera, color_silhouette_renderer)
    if args.model_occlusions:
        texture_modality.ModelOcclusions(focused_color_depth_renderer)
    link.AddModality(texture_modality)

optimizer = pym3t.Optimizer(args.body_name+'_optimizer', link)
tracker.AddOptimizer(optimizer)

#----------------
# Intialize object pose
link2world_pose = np.array([ 1, 0,  0, 0,
                             0, 0, -1, 0,
                             0, 1,  0, 0.456,
                             0, 0,  0, 1 ]).reshape((4,4))
dR_l = quaternion.as_rotation_matrix(quaternion.from_rotation_vector([0.2,0,0.0]))
link2world_pose[:3,:3] = link2world_pose[:3,:3] @ dR_l
#----------------

ok = tracker.SetUp()
if not ok:
	raise ValueError('tracker SetUp failed')

tracker.n_update_iterations = 2
tracker.n_update_iterations = 5
print(tracker.n_corr_iterations)
print(tracker.n_update_iterations)


tracking = False
i = 0
while(True): 
	ret, frame = vid.read()
	color_camera.image = frame
	ok = tracker.UpdateCameras(True)  # poststep verifying the images have been properly setup
	if not ok:
		raise ValueError('Something is wrong with the provided images')
      
	k = cv2.waitKey(1)
	if k == ord('d'):
		print('Init object pose')
		body.body2world_pose = link2world_pose  # simulate external initial pose
		# body.link2world_pose = link2world_pose  # no effect
		tracking = False
	if k == ord('x'):
		tracking = True
		print('StartTracking')
	if k == ord('q'):
		break
	if tracking:
		tracker.ExecuteTrackingStep(i)
	i += 1
	color_viewer.UpdateViewer(i)

vid.release() 
cv2.destroyAllWindows() 
