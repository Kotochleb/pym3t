"""
python run_image_per_image.py --use_region --use_depth --use_texture --measure_occlusions -b obj_000014 -m <path/to/obj/dir> -i <path/to/image/dir> -c config/cam_d435_640.yaml -n 10 -s
"""

import cv2
import yaml
import glob
import time
import argparse
import numpy as np
import quaternion
from pathlib import Path

import pym3t

def inv_SE3(T):
    """
    Inverse of an SE(3) 4x4 array
    """
    Tinv = np.eye(4)
    Tinv[:3,:3] = T[:3,:3].T
    Tinv[:3,3] = -T[:3,:3].T@T[:3,3]
    return Tinv


def tq_to_SE3(t, q):
    """
    t: translation as list or array
    q: quaternion as list or array, expected order: xyzw
    out: 4x4 array representing the SE(3) transformation
    """
    T = np.eye(4)
    T[:3,3] = t
    # np.quaternion constructor uses wxyz convention
    quat = np.quaternion(q[3], q[0], q[1], q[2]).normalized()
    T[:3,:3] = quaternion.as_rotation_matrix(quat)
    return T


def parse_script_input():
    parser = argparse.ArgumentParser(
        prog='run_image_per_image',
        description='Run the m3t tracker image per image reading from rgb/depth folder'
    )

    parser.add_argument('-b', '--body_name',  dest='body_name',  type=str, required=True, help='Name of the object to track. need to match')
    parser.add_argument('-m', '--models_dir', dest='models_dir', type=str, required=True, help='Path to directory where object model file {body_name}.obj is stored')
    parser.add_argument('-c', '--cam_path',   dest='cam_path', type=str, required=True, help='Path to camera intrinsics file')
    parser.add_argument('-i', '--imgs_dir',   dest='imgs_dir',   type=str, required=True, help='Path to directory where "rbg*" and "depth*" named images are stored')
    parser.add_argument('-n', '--nb_img_load',     dest='nb_img_load',   type=int, default=-1)
    parser.add_argument('-s' '--stop_at_each_img', dest='stop_at_each_img',   action='store_true', default=False)
    parser.add_argument('--scale_geometry', dest='scale_geometry', default=0.001, type=float, required=False, help='Scale factor to convert model geometry to meters.')
    parser.add_argument('--tmp_dir',    dest='tmp_dir',    type=str, default='tmp', help='Directory to store preprocessing files generated by the tracker.')
    parser.add_argument('--use_region', dest='use_region', action='store_true', default=False)
    parser.add_argument('--use_depth', dest='use_depth', action='store_true', default=False)
    parser.add_argument('--use_texture', dest='use_texture', action='store_true', default=False)
    parser.add_argument('--use_depth_viewer', dest='use_depth_viewer', action='store_true', default=False)
    parser.add_argument('--model_occlusions', dest='model_occlusions', action='store_true', default=False)
    parser.add_argument('--measure_occlusions', dest='measure_occlusions', action='store_true', default=False)

    return parser.parse_args()

args = parse_script_input()
tmp_dir = Path(args.tmp_dir)
tmp_dir.mkdir(exist_ok=True)

# synchronize_cameras: to be able to print elapsed time
tracker = pym3t.Tracker('tracker', synchronize_cameras=False)
renderer_geometry = pym3t.RendererGeometry('renderer geometry')

with open(args.cam_path, 'r') as f:
    cam = yaml.load(f.read(), Loader=yaml.UnsafeLoader)

color_camera = pym3t.DummyColorCamera('cam_color')
color_camera.color2depth_pose = tq_to_SE3(cam['trans_d_c'], cam['quat_d_c_xyzw'])
color_camera.intrinsics = pym3t.Intrinsics(**cam['intrinsics_color'])

depth_camera = pym3t.DummyDepthCamera('cam_depth')
depth_camera.depth2color_pose = inv_SE3(color_camera.color2depth_pose)
depth_camera.intrinsics = pym3t.Intrinsics(**cam['intrinsics_depth'])


# Most time is spent on rendering (tested without GPU: ~15 ms for both, 8 for color only)
color_viewer = pym3t.NormalColorViewer('color_viewer', color_camera, renderer_geometry)
tracker.AddViewer(color_viewer)
if args.use_depth and args.use_depth_viewer:
    depth_viewer = pym3t.NormalDepthViewer('depth_viewer_name', depth_camera, renderer_geometry)
    tracker.AddViewer(depth_viewer)

# Setup body model and properties
obj_model_path = Path(args.models_dir) / f'{args.body_name}.obj'
print(f'Loading object {obj_model_path}')
body = pym3t.Body(
    name=args.body_name,
    geometry_path=obj_model_path.as_posix(),
    geometry_unit_in_meter=args.scale_geometry,
    geometry_counterclockwise=1,
    geometry_enable_culling=1,
    geometry2body_pose=np.eye(4)
)
renderer_geometry.AddBody(body)

# Set up link (m3t handles polyarticulated systems, here we have only one link corresponding to the object)
link = pym3t.Link(args.body_name + '_link', body)

# Shared renderer between region and texture 
if args.model_occlusions and (args.use_region or args.use_texture):
    focused_color_depth_renderer = pym3t.FocusedBasicDepthRenderer('focused_color_depth_renderer', renderer_geometry, color_camera)
    focused_color_depth_renderer.AddReferencedBody(body)

# Region Modality
if args.use_region:
    region_model_path = tmp_dir / (args.body_name + '_region_model.bin')
    region_model = pym3t.RegionModel(args.body_name + '_region_model', body, region_model_path.as_posix())
    region_modality = pym3t.RegionModality(args.body_name + '_region_modality', body, color_camera, region_model)
    if args.model_occlusions:
        region_modality.ModelOcclusions(focused_color_depth_renderer)
    if args.measure_occlusions and args.use_depth:
        region_modality.MeasureOcclusions(depth_camera)
    link.AddModality(region_modality)

# Depth Modality
if args.use_depth:
    depth_model_path = tmp_dir / (args.body_name + '_depth_model.bin')
    depth_model = pym3t.DepthModel(args.body_name + '_depth_model', body, depth_model_path.as_posix())
    depth_modality = pym3t.DepthModality(args.body_name + '_depth_modality', body, depth_camera, depth_model)
    if args.model_occlusions:
        focused_depth_depth_renderer = pym3t.FocusedBasicDepthRenderer('focused_depth_depth_renderer', renderer_geometry, depth_camera)
        focused_depth_depth_renderer.AddReferencedBody(body)
        depth_modality.ModelOcclusions(focused_depth_depth_renderer)
    if args.measure_occlusions and args.use_depth:
        depth_modality.MeasureOcclusions()
    link.AddModality(depth_modality)

# Texture Modality
if args.use_texture:
    # Texture modality does not require a model contrary to region and depth (for sparse view precomputations)
    color_silhouette_renderer = pym3t.FocusedSilhouetteRenderer('color_silhouette_renderer', renderer_geometry, color_camera)
    color_silhouette_renderer.AddReferencedBody(body)
    texture_modality = pym3t.TextureModality(args.body_name + '_texture_modality', body, color_camera, color_silhouette_renderer)
    if args.model_occlusions:
        texture_modality.ModelOcclusions(focused_color_depth_renderer)
    if args.measure_occlusions and args.use_depth:
        texture_modality.MeasureOcclusions(depth_camera)
    link.AddModality(texture_modality)

optimizer = pym3t.Optimizer(args.body_name+'_optimizer', link)
tracker.AddOptimizer(optimizer)

#----------------
# Intialize object pose
link2world_pose = np.array([ 1, 0,  0, 0,
                             0, 0, -1, 0,
                             0, 1,  0, 0.456,
                             0, 0,  0, 1 ]).reshape((4,4))
dR_l = quaternion.as_rotation_matrix(quaternion.from_rotation_vector([0.2,0,0.0]))
link2world_pose[:3,:3] = link2world_pose[:3,:3] @ dR_l
#----------------

ok = tracker.SetUp()

imgs_dir = Path(args.imgs_dir)
if not imgs_dir.exists():
    print(f'Wrong path to image directory: {imgs_dir}')
# read images from disk
color_names = sorted(glob.glob((imgs_dir / 'color*').as_posix()))
depth_names = sorted(glob.glob((imgs_dir / 'depth*').as_posix()))

# LIMIT nb images
color_names = color_names[:args.nb_img_load]
depth_names = depth_names[:args.nb_img_load]
print(f'{len(color_names)} images to load')
print('\n------\nPress q to quit during execution')
print('Press any key to step to next image')

# load images from disk
color_read_flags = cv2.IMREAD_COLOR + cv2.IMREAD_ANYDEPTH
img_bgr_lst = [cv2.imread(name, color_read_flags) for name in color_names]  # loads a dtype=uint8 array
depth_read_flags = cv2.IMREAD_GRAYSCALE + cv2.IMREAD_ANYDEPTH
img_depth_lst = [cv2.imread(name, depth_read_flags) for name in depth_names]  # loads a dtype=uint8 array

# tracker.n_update_iterations = 2
tracker.n_update_iterations = 5
print(tracker.n_corr_iterations)
print(tracker.n_update_iterations)

SLEEP = int(1000/30)  # frames at 30 Hz

# Simulate one iteration of Tracker::RunTrackerProcess for loop
for i, (img_bgr, img_depth) in enumerate(zip(img_bgr_lst, img_depth_lst)):
    t1 = time.time()
    print('\nIteration: ', i)
    # 1) Update camera image -> replaces a call to the camera UpdateImage method (which does nothing for Dummy(Color|Depth)Camera) 
    color_camera.image = img_bgr
    depth_camera.image = img_depth
    ok = tracker.UpdateCameras(True)  # poststep verifying the images have been properly setup
    if not ok:
        raise ValueError('Something is wrong with the provided images')

    if i == 0:
        # 2) Use external init to update initial object pose
        body.body2world_pose = link2world_pose  # simulate external initial pose
        # link.link2world_pose = link2world_pose  # no effect
        
    # 3) One tracking cycle
    t = time.time()
    tracker.ExecuteTrackingStep(i)
    print('ExecuteTrackingCycle (ms)', 1000*(time.time() - t))
    print(link.link2world_pose)

    # 4) Render results
    t = time.time()
    color_viewer.UpdateViewer(i)
    if args.use_depth and args.use_depth_viewer:
        depth_viewer.UpdateViewer(i)
    print('Updating viewers took (ms)', 1000*(time.time() - t))
    
    if args.stop_at_each_img:
        k = cv2.waitKey(0)
    else:
        delay = time.time() - t1
        sleep = max(1, SLEEP - int(1000*delay))
        k = cv2.waitKey(sleep)
    if k == ord('q'):
        break
