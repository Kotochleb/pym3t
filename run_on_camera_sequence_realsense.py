"""
Frames of reference:
- w = World: either = C or D, the camera is fixed wrt. the World
- c = Camera: frame of the RGB camera (origin = optical center, XYZ = Right-Down-Front)
- d = Depth: frame of the Depth camera (//)
- b = Body: frame associated to one of the tracked objects 
- g = Geometry: frame associated with the origin of the body geometric model (may be != b)

For the following comments, we adopt the notation: a 3D point p expressed in frames a and b by
the transformation T_a_b: p_a = T_a_b * p_b

"""

import numpy as np
import argparse
import pym3t
from pathlib import Path



def parse_script_input():
    parser = argparse.ArgumentParser(
        prog='run_image_per_image',
        description='Run the m3t tracker image per image on '
    )

    parser.add_argument('-b', '--body_name',  dest='body_name',  type=str, required=True, help='Name of the object to track. need to match')
    parser.add_argument('-m', '--models_dir', dest='models_dir', type=str, required=True, help='Path to directory where object model file .obj is stored')
    parser.add_argument('--config_dir', dest='config_dir', type=str, default='config', help='Path to directory where <body_name>.yaml and static_detector.yaml files are stored')
    parser.add_argument('--tmp_dir',    dest='tmp_dir',    type=str, default='tmp', help='Directory to store preprocessing files generated by the tracker.')
    parser.add_argument('--detector_file', dest='detector_file', type=str, default='static_detector.yaml')
    parser.add_argument('--use_depth', dest='use_depth', action='store_true', default=False)
    parser.add_argument('--model_occlusions', dest='model_occlusions', action='store_true', default=False)

    return parser.parse_args()


args = parse_script_input()

body_name = args.body_name
models_dir = Path(args.models_dir)
config_dir = Path(args.config_dir)
tmp_dir = Path(args.tmp_dir)
tmp_dir.mkdir(exist_ok=True)
detector_file = args.detector_file
use_depth = args.use_depth
model_occlusions = args.model_occlusions


# synchronize_cameras: to be able to print elapsed time
tracker = pym3t.Tracker('tracker', synchronize_cameras=False)

renderer_geometry = pym3t.RendererGeometry('renderer geometry')

color_camera = pym3t.RealSenseColorCamera('realsense_color')
depth_camera = pym3t.RealSenseDepthCamera('realsense_depth')

# Most time is spent on rendering (tested without GPU: ~15 ms for both, 8 for color only)
depth_viewer = pym3t.NormalDepthViewer('depth_viewer_name', depth_camera, renderer_geometry, 0.3, 1.0)
# tracker.AddViewer(depth_viewer)

color_viewer = pym3t.NormalColorViewer('color_viewer', color_camera, renderer_geometry)
tracker.AddViewer(color_viewer)

metafile_path = models_dir / (body_name+'.yaml')
body = pym3t.Body(body_name, metafile_path.as_posix())
renderer_geometry.AddBody(body)

region_model_path = tmp_dir / (body_name + '_region_model.bin')
region_model = pym3t.RegionModel(body_name + '_region_model', body, region_model_path.as_posix())
depth_model_path = tmp_dir / (body_name + '_depth_model.bin')
depth_model = pym3t.DepthModel(body_name + '_depth_model', body, depth_model_path.as_posix())

# Runtime renderers for occlusion handling and texture modality
color_depth_renderer = pym3t.FocusedBasicDepthRenderer('color_depth_renderer', renderer_geometry, color_camera)
depth_depth_renderer = pym3t.FocusedBasicDepthRenderer('depth_depth_renderer', renderer_geometry, depth_camera)
color_silhouette_renderer = pym3t.FocusedSilhouetteRenderer('color_silhouette_renderer', renderer_geometry, color_camera)
color_depth_renderer.AddReferencedBody(body)
depth_depth_renderer.AddReferencedBody(body)
color_silhouette_renderer.AddReferencedBody(body)

region_modality = pym3t.RegionModality(body_name + '_region_modality', body, color_camera, region_model)
depth_modality = pym3t.DepthModality(body_name + '_depth_modality', body, depth_camera, depth_model)
texture_modality = pym3t.TextureModality(body_name + '_texture_modality', body, color_camera, color_silhouette_renderer)


if model_occlusions:

    """
    FocusedRenderer: interface with OpenGL, render only part of the image where tracked objects are present 
                    -> projection matrix is recomputed each time a new render is done (contrary to FullRender)
    Used for render based occlusion handling.
    """
    # We need 2 renderers because depth and color are slightly not aligned

    region_modality.ModelOcclusions(color_depth_renderer)
    depth_modality.ModelOcclusions(depth_depth_renderer)
    texture_modality.ModelOcclusions(color_depth_renderer)


# Set up link
link = pym3t.Link(body_name + '_link', body)
link.AddModality(region_modality)
link.AddModality(depth_modality)
link.AddModality(texture_modality)

optimizer = pym3t.Optimizer(body_name+'_optimizer', link)
tracker.AddOptimizer(optimizer)

detector_path = config_dir / detector_file
link2world_pose = np.array([ 0, 1,  0, 0,
                             0, 0, -1, 0,
                             -1, 0,  0, 0.456,
                             0, 0,  0, 1 ]).reshape((4,4))
detector = pym3t.StaticDetector('static_detector', optimizer, link2world_pose, False)
tracker.AddDetector(detector)

ok = tracker.SetUp()
print('tracker.SetUp ok: ', ok)
names_detecting = {body_name}
names_starting = {body_name}
tracker.RunTrackerProcess(execute_detection=True, start_tracking=True, names_detecting=names_detecting, names_starting=names_starting)
